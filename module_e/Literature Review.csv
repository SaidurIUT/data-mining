paper_id,title,authors,year,venue,main_technique,summary,relevance_to_system
1,Cross-Language Information Retrieval: A Survey,"Nie, J.Y.",2010,Foundations and Trends in IR,Query/Document Translation + Interlingual,"Comprehensive CLIR survey covering query translation, document translation, and interlingual approaches. Highlights translation ambiguity and OOV term handling challenges.","Our system uses interlingual approach via LaBSE embeddings, avoiding explicit translation"
2,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer,"Artetxe, M. & Schwenk, H.",2019,TACL,BiLSTM Encoder + Translation Ranking Loss,"Introduces LASER for 93 languages using shared encoder. Enables zero-shot cross-lingual transfer via sentence embeddings mapped to common space.","Foundation for our LaBSE-based semantic search enabling Englishâ†’Bangla retrieval"
3,XLM-RoBERTa: Unsupervised Cross-lingual Representation Learning at Scale,"Conneau, A. et al.",2020,ICLR,Multilingual MLM on CommonCrawl,"Scales multilingual pretraining to 100 languages using 2.5TB data. Shows model capacity mitigates 'curse of multilinguality'.","Informs why modern multilingual models work well for Bangla (lower-resource)"
4,Language-agnostic BERT Sentence Embedding (LaBSE),"Feng, F. et al. (Google)",2020,arXiv,Dual Encoder + MLM + Translation Loss,"Combines MLM pretraining with translation ranking for 109 languages. State-of-the-art on Tatoeba cross-lingual retrieval.","Core embedding model in our semantic search module"
5,Okapi BM25: A Non-Binary Model,"Robertson, S.E. et al.",1994,TREC,TF-IDF with Length Normalization,"Probabilistic ranking using term frequency saturation and document length normalization. Strong baseline even today.","Our BM25 module for lexical matching, especially effective for same-language queries"
